{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this project I will explore the following dataset:\n",
    "### *Telco Customer Churn*, BlastChar, 2017\n",
    "### https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "\n",
    "### This is a sample dataset from IBM. It contains customer data from a phone and internet company for over 7000 customers.\n",
    "\n",
    "\n",
    "### My project focus is to analyze the factors that contribute to customer churn. Customer churn in this context, refers to customers who have left the company in the last month.  \n",
    "\n",
    "### I plan to create visualizations to understand customer churn and what variables contribute to it, in this dataset. Also, I will prototype machine learning models to devlop a high performing model to predict whether a customer has churned or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "full_data = pd.read_csv('customer_churn_data.csv')\n",
    "# Count nulls\n",
    "print(full_data.isnull().sum())\n",
    "\n",
    "# Get rid of ' ' value(s) in TotalCharges\n",
    "clean_data = full_data[full_data['TotalCharges'] != ' ']\n",
    "\n",
    "# Drop customer ID column\n",
    "clean_data = clean_data.drop(columns=['customerID'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above, I checked for the prescence of any null values. As expected, since this is a sample dataset, there are no null values. However, in the TotalCharges column, which tracks a customer's total charges to their account, contains empty string values. So, I got rid of the rows with the string ' '.\n",
    "\n",
    "### Also, I dropped the customerID column, since it will not be relevant to my future analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of numeric outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure continous variables are of type float\n",
    "clean_data[['tenure', 'MonthlyCharges', 'TotalCharges']] = clean_data[['tenure', 'MonthlyCharges', 'TotalCharges']].astype(float)\n",
    "\n",
    "# Create function to eliminate rows with continous outliers\n",
    "def outlier_zscoreMethod(dataset, variables, threshold):\n",
    "    # Make copy of original data\n",
    "    filtered_dataset = dataset.copy()\n",
    "    for variable in variables:\n",
    "        # Calculate z scores for current variable\n",
    "        z_scores = stats.zscore(filtered_dataset[variable])\n",
    "        # Create mask that will be used to filter outliers out\n",
    "        outlier_mask = abs(z_scores) > threshold\n",
    "        # Apply mask\n",
    "        filtered_dataset = filtered_dataset[~outlier_mask]\n",
    "    return filtered_dataset\n",
    "'''\n",
    "This function gets rid of outliers in a dataframe(only for continous variables) using the z-score method.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "dataset: 2D Pandas dataframe\n",
    "A dataset in df form.\n",
    "\n",
    "variables: list of string elements\n",
    "Column names of a dataframe which are quantitative variables.\n",
    "\n",
    "threshold: float\n",
    "Number representing how many standard deviations away an observation has to be from the mean for it to be considered an outlier.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "filtered_dataset: 2D Pandas dataframe\n",
    "New dataset, which no longer contains rows with outliers.\n",
    "'''\n",
    "\n",
    "# List of continous variables\n",
    "continous_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "# Call function to get reid of outliers in continous variables\n",
    "clean_data = outlier_zscoreMethod(clean_data, continous_features, 3.0)\n",
    "# No outliers were removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, I created a function to get rid of outliers in the dataset. This function applies the z-score method. Using the function, I tried to eliminate continous values greater than 3 z-scores. No values exceeded this threshold, so data was not removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies for categorical variables\n",
    "data = pd.get_dummies(clean_data, columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn'], drop_first=True)\n",
    "\n",
    "# Convert categorical variables to int data type\n",
    "data = data.apply(lambda x: x.astype(int) if x.name not in continous_features else x)\n",
    "\n",
    "# Drop redundant features\n",
    "data = data.drop(columns=['MultipleLines_No phone service', 'OnlineSecurity_No internet service', 'OnlineBackup_No internet service', 'DeviceProtection_No internet service', 'TechSupport_No internet service', 'StreamingTV_No internet service', 'StreamingMovies_No internet service'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above, I converted all categorical variables to dummy variables- as int data types. I also dropped redundant binary features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, I will make some visualizations to help understand customer churn in this dataset- its frequency, and what factors contribute to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are a couple of functions for future plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_average(dataset, continuous_var_name, title, y_axis_label):\n",
    "    # Group data by churn and then calculate average of continuous variable \n",
    "    means = dataset.groupby('Churn_Yes')[continuous_var_name].mean()\n",
    "\n",
    "    # Plot bar plot\n",
    "    means.plot(kind='bar', color=['lightgreen', 'purple'])\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Churn Status')\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Customize x-axis tick labels\n",
    "    plt.xticks(ticks=[0, 1], labels=['Stayed', 'Churned'], rotation=0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "'''\n",
    "This function creates a bar plot, where the x-axis is churn status, and the y-axis is the average of a continous variable.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "dataset: 2D Pandas dataframe\n",
    "A dataset in df form.\n",
    "\n",
    "continuous_var_name: string\n",
    "Column name of dataframe which is a continuous variable(variable on y-axis).\n",
    "\n",
    "title: string\n",
    "Title of the plot.\n",
    "\n",
    "y-axis_label: string\n",
    "Label of the y-axis of the plot.\n",
    "\n",
    "Result\n",
    "-------\n",
    "A bar plot created by matplotlib.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_percentage(dataset, binary_var_name, title, y_axis_label):\n",
    "    # Calculate percent of binary variable for each class\n",
    "    percent_churned_yes = (dataset[dataset['Churn_Yes'] == 1][binary_var_name].mean())*100\n",
    "\n",
    "    # Calculate percent of customers who do not churn(stay)\n",
    "    percent_churned_no = 100 - percent_churned_yes\n",
    "\n",
    "    # Plot\n",
    "    plt.bar(['Stayed', 'Churned'], [percent_churned_no, percent_churned_yes], color=['skyblue', 'orange'])\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "'''\n",
    "This function creates a bar plot, where the x-axis is churn status, and the y-axis is a different binary \n",
    "variable(represented as a percentage).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "dataset: 2D Pandas dataframe\n",
    "A dataset in df form.\n",
    "\n",
    "binary_var_name: string\n",
    "Column name of dataframe which is a binary variable(variable on y-axis).\n",
    "\n",
    "title: string\n",
    "Title of the plot.\n",
    "\n",
    "y-axis_label: string\n",
    "Label of the y-axis of the plot.\n",
    "\n",
    "Result\n",
    "-------\n",
    "A bar plot created by matplotlib.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, I will look at how often customer churn occures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent of customers who churn\n",
    "percent_churn = (np.mean(data['Churn_Yes']))*100\n",
    "\n",
    "# Calculate percent of customers who do not churn(stay)\n",
    "percent_stay = 100 - percent_churn\n",
    "\n",
    "# Plot\n",
    "plt.bar(['Churn', 'Stay'], [percent_churn, percent_stay], color=['skyblue', 'orange'])\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title('Customer Churn Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that about 26 percent of customers churn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to predict whether a customer has churned, it is important to identify variables in this dataset that are good predictors. To do this, I will fit a basic decision tree on all the predictor variables to predict churn. Then, I will find the feature importances for each variable. These are calculated by ranking each variable in terms of how well they contributed to splitting nodes in the tree, in order to get closer to predicting churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get independent variables\n",
    "X = data.drop(columns=['Churn_Yes'])\n",
    "\n",
    "# Get dependent variable\n",
    "y = data['Churn_Yes']\n",
    "\n",
    "# Initialize dt classifier\n",
    "dt = DecisionTreeClassifier(random_state=10)\n",
    "# Fit model\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Calculate the importance level of each feature in X_data1 for the dt clasifier\n",
    "feature_ranking = dt.feature_importances_\n",
    "\n",
    "# Create dataframe of feature importance with the corresponding features\n",
    "feature_ranking_df = pd.DataFrame({'Feature': X.columns, 'Importance_Level': feature_ranking})\n",
    "#print(featureImportance_df)\n",
    "\n",
    "# Sort the df in a descending order\n",
    "feature_ranking_df_sorted = feature_ranking_df.sort_values(by='Importance_Level', ascending=False)\n",
    "\n",
    "# Print the sorted df\n",
    "print(feature_ranking_df_sorted) # Income is the most important feature followed by recency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above variable importance ranking, the following variables are most important to predicting customer churn: Tenure, total and monthly charges, whether the customer uses fiber optic and/or electronic checks, and whether the customer is male."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now take a look more closely on the relationships between these variables and customer churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_average(dataset=data, continuous_var_name='tenure', title='Mean Tenure (Months Stayed in Company) By Churn Status', y_axis_label='Average Tenure(months)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above plot, we see the average tenure for customers who did not churn is about 37 months(about 3.08 years), while the average tenure for customers who did churn is about 17 months(about 1.42 years). This is a significant difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_average(dataset=data, continuous_var_name='TotalCharges', title='Mean Total Charges By Churn Status', y_axis_label='Average Total Charges')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As seen above, the average total charges for customers who have churned is about $1000 less than those who have stayed in the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_average(dataset=data, continuous_var_name='MonthlyCharges', title='Mean Monthly Charges By Churn Status', y_axis_label='Average Monthly Charges')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interestingly, customers who have churned have higher average monthly charges- about $73. And customers who have not churned have an average of about $61. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_percentage(dataset=data, binary_var_name='InternetService_Fiber optic', title='Fiber Optic Users by Churn Status', y_axis_label='Fiber Optic Percentage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see here that a large majority of customers who have churned use fibor optic as their internet service providor(almost 70%)- a little over double the customers who have not churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_percentage(dataset=data, binary_var_name='PaymentMethod_Electronic check', title='Electronic Check Users by Churn Status', y_axis_label='Electronic Check Percentage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we see that a majority of churned customers use electronic checks(about 58%) as opposed to customers who did not churn(about 42%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_percentage(dataset=data, binary_var_name='gender_Male', title='Gender by Churn Status', y_axis_label='Gender Male Percentage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, we see the gender statistics for customers. Of the customers who did not churn, a little over 50% of them are male. Of the customers who did churn, about 50% of them are male. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the goal is to accurately predict whether a customer has churned or not, this problem falls under classification. Since the churn variable is binary(and therefore categorical), I will prototype a few classification models to classify churn- logistic regression, random forest, and boosting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First I will make a function to calculate cross validated error of these models. This will help to determine the most accurate model, on average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cv_accuracy(model, X, y, cv=5):\n",
    "    # Calculate cv accuracies\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')  \n",
    "    return scores.mean()\n",
    "'''\n",
    "This function calculates cross validated error of some scikit-learn models\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "model: object\n",
    "A scikit-learn model object\n",
    "\n",
    "X: 2D Pandas dataframe\n",
    "Predictor variables for model training.\n",
    "\n",
    "y: 1D Pandas Series\n",
    "Response variable for model training.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "Scores.mean(): float\n",
    "Average accuracy from cross validation.\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start I will use Logistic Regression. With each model type, I will use three sets of the dataset for X- The first is the entire X matrix, which contains of the predictors. The next subset, X_2 will contain the top six variables from the feature importance ranking. Lastly, I will use X_3, which contains the top 4 variables, since there is a big doproff in feature importance after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# Scale numeric data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[continous_features] = scaler.fit_transform(X[continous_features])\n",
    "\n",
    "# Get X subsets for models(scaled)\n",
    "X_2_scaled = X_scaled[['tenure', 'MonthlyCharges', 'TotalCharges', 'InternetService_Fiber optic', 'PaymentMethod_Electronic check', 'gender_Male']]\n",
    "X_3_scaled = X_2_scaled.drop(columns=['PaymentMethod_Electronic check', 'gender_Male'])\n",
    "# Create list of dfs\n",
    "X_scaled_list = [X_scaled, X_2_scaled, X_3_scaled]\n",
    "\n",
    "# Get X subsets for models(not scaled)\n",
    "X_2 = X[['tenure', 'MonthlyCharges', 'TotalCharges', 'InternetService_Fiber optic', 'PaymentMethod_Electronic check', 'gender_Male']]\n",
    "X_3 = X_2.drop(columns=['PaymentMethod_Electronic check', 'gender_Male'])\n",
    "# Create list of dfs\n",
    "X_list = [pd.DataFrame(X), pd.DataFrame(X_2), pd.DataFrame(X_3)]\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr = LogisticRegression(random_state=10)\n",
    "\n",
    "# Calculate cross-validated accuracy for models\n",
    "for x in X_scaled_list:\n",
    "    print(f\"Cross-validated Accuracy: {calculate_cv_accuracy(lr, x, y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of all the X matrices used, the one with all the predictors had the highest cross validated accuracy for logistic regression(when predicting customer churn)- 80.33%. This means that the best logistic regression model here is, on average, 80.33% accurate when predicting unseen data.The less predictors I included, the lower the cross validated accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, is Random Forest, and Gradient Boosting. For these models, I will use the first subset- all of the predictors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For random forest, I will use the grid search method to find the optimal number trees, max depth, and max_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random forest model\n",
    "rf = RandomForestClassifier(random_state=10)\n",
    "\n",
    "# Create prameter grid for rf\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['auto', 'sqrt', None]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X, y)\n",
    "\n",
    "# Find best model\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(f\"Best hyperparameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "# Obtain best CV accuracy\n",
    "cv_accuracy_rf = grid_search_rf.best_score_\n",
    "print(f\"Best cross-validation accuracy: {cv_accuracy_rf}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After finding optimal parameters, the cross-validated accuracy for predicting customer churn is about 80.49%. This is slightly less than the accuracy of logistic regression. This means that this specific random forest model is, on average, 80.49% accurate when predicting unseen data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, for the gradiant boosting tree model I will do a similar grid search procedure, except, instead of optimizing max_features, I will optimize learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid for GradientBoostingClassifier\n",
    "param_grid_gbc = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation on the dataset\n",
    "grid_search_gbc = GridSearchCV(estimator=gbc, param_grid=param_grid_gbc, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search_gbc.fit(X, y)\n",
    "\n",
    "# Retrieve the best model \n",
    "best_gbc = grid_search_gbc.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(f\"Best hyperparameters: {grid_search_gbc.best_params_}\")\n",
    "\n",
    "# Obtain best CV accuracy\n",
    "cv_accuracy_gbc = grid_search_gbc.best_score_\n",
    "print(f\"Best cross-validation accuracy: {cv_accuracy_gbc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After finding optimal parameters, the cross-validated accuracy for predicting customer churn is about 80.46%. This is slightly less than the accuracy of the random forest model, but slighly better than the logistic regression model. This means that this specific boosting model is, on average, 80.46% accurate when predicting unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall, the random forest had the highest accuracy for predicting customer churn on unseen data- 80.49% accurate. The gradient boosting model came in a close second- 80.46% accurate. Logistic regression had the lowest average accuracy- 80.33% accurate. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These models have many practical applications. They can be used to predict whether or not a customer has a high chance of leaving the company. Marketing campaigns and promotions can be targeted to those who have already churned, or are at a high risk of churn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds385",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6eb6178224bd9e53f60d2d51f701ad3b00c913d0670cd2bcae23b7788028ba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
